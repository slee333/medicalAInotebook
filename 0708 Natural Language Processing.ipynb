{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 텍스트 분석\n",
    "\n",
    "## 텍스트 분석의 종류\n",
    "\n",
    "정보 추출 (Information retrieval): 문서 내의 정형 데이터를 추출하는 작업  \n",
    "문서 분류 (Text classification): 문서들을 특정 분류 체계에 EKfk qnsfb  \n",
    "감성 분석 (Sentiment analysis): 문서에 내포된 감정과 의견을 추출  \n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "# 2. 토큰화 (Tokenization)\n",
    "\n",
    "**Definition:** 해당 데이터를 의미 단위로 구분하는 전처리 과정. corpus에서 token으로 나누는 과정.  \n",
    "- Type: 단어, 요소, 어간\n",
    "- Token: 텍스트 내부의 원소\n",
    "\n",
    "전처리 과정은 토큰화 (tokenization), 정제 (cleaning), 정규화 (normalization)이 필요하다.\n",
    "\n",
    "### 2.1 토큰화의 유형\n",
    "---\n",
    "토큰의 기준을 단어로 하는 경우 (영어)  \n",
    "한국어의 경우 단순 공백 기준이 아닌 품사 기준으로 토큰화   \n",
    "\n",
    "*ex) 한국어를 토큰화하는 예시 문장입니다  \n",
    "-> 한국어, 를, 토큰화, 하다, 예시, 문장, 입, 니다*\n",
    "\n",
    "\n",
    "# 3. 표제어 추출 (Lemmatization)\n",
    "---\n",
    "문장 속에서 다양현 형태로 변형된 단어의 표제어(lemma)를 찾는 작업.  \n",
    "\n",
    "- 표제어: am, are, is는 전부 be라는 표제어를 가짐\n",
    "\n",
    "ex) stemming -> stem  \n",
    "\n",
    "어간 추출 (Stemming)과는 다름.\n",
    "- Stemming: flies -> fly. 섬세한 작업이 아니다.\n",
    "- Lemmatization: flies가 날다인지 파리인지 파악. 즉 단어의 품사 정보를 보존함.\n",
    "\n",
    "### 3.1 Porter algorithm\n",
    "  \n",
    "  대표적인 어간 추출 알고리즘. 하지만 어간 추출의 경우 결과가 사전에 존재하지 않는 경우가 많다.\n",
    "  \n",
    "  \n",
    "# 4. 형태소 분석\n",
    "---\n",
    "토큰의 품사 정보를 추출하는 작업.  \n",
    "POS-tagging 이라고도 불림 (Part-of-Speech Tagging)  \n",
    "\n",
    "### 4.1 Open class vs closed class:\n",
    "- Closed class: 상대적으로 고정되어 있는 셋\n",
    "    - 전치사: of, in, at\n",
    "    - 조동사: may, have, can, will, must\n",
    "    - 대명사: I, you, she\n",
    "    - 주로 문장 내에서 문법적 역할을 하는 단어\n",
    "- Open class:\n",
    "    - Noun, Verbs Adjectives 등이 있음\n",
    "    - 명사: Sejoing Univ, Samsung,\n",
    "    - 동사: Listen, etc\n",
    "    \n",
    "영어 형태소 분석기는 Stanford POS-tagger나 NLTK POS-tagger 쓰고 한국어 형태소 분석기는 Hannanum, kkma, momoran, Twitter 등을 쓴다.  \n",
    "\n",
    "\n",
    "# 5. 정보 추출 (Information retrieval)\n",
    "---\n",
    "\n",
    "**Definition** 구조화되지 않은 문서 군집 내에서 필요한 정보를 포함하는 문서를 찾아냄.\n",
    "\n",
    "### 5.1 Document Term matrix 문서 단어 행렬\n",
    "\n",
    "각 단어가 각 문서에 등장하는지 유무를 vector로 나타냄.  \n",
    "\n",
    "### 5.2 Bag of words 모델\n",
    "\n",
    "문서 내에 등장하는 단어들의 순서를 무시하고 벡터로 표현\n",
    "\n",
    "### 5.3 Term frequency\n",
    "\n",
    "${tf_(t,d)}$는 특정 단어 t가 특정 문서 d에 등장하는 빈도.   \n",
    "문서의 관련도와 타당성은 **term frequency**와 비례하여 증가하지 **않음**.  \n",
    "\n",
    "**${w_{t,d} = 1 + \\log tf_{t,d} }$**\n",
    "if tf_t,d > 0\n",
    "\n",
    "tf값이 클수록 문서에 많이 등장하지만, 정보가 부족한 가능성일 수도 있음. **드물게 등장하는 단어가 더 중요한 정보를 가지고 있을 확률이 높아 가중치를 부여해야 함.**\n",
    "\n",
    "\n",
    "\n",
    "### 5.4 Document Frequency\n",
    "${df_t}$는 document frequency, 즉 단어 t가 등장한 문서의 수.  \n",
    "- ${df_t}$는 단어 t의 중요도의 역수와 같다.\n",
    "- ${idf_t = \\log \\frac{N}{df_t}}$ : idf는 단어 t의 중요도를 나타내는 척도이다. N = 문서 군집에 속하는 문서의 갯수\n",
    "\n",
    "### 5.5 TF-IDF\n",
    "TF-IDF (Term frequency - Inverse Document Frequency): TF와 IDF의 곲.\n",
    "- 문서군 내에서 특정 단어의 중요도를 수치화한 값\n",
    "- 문서 내에서 단어의 빈도 (tf)가 올라갈 수록 커지며\n",
    "- 문서군 내에서 단어가 희박하게 등장할수록 (idf) 커짐\n",
    "- ${w_{t,d} = (1+\\log tf)*\\log \\frac{N}{df_t}}$  \n",
    "\n",
    "### 5.6 정보 추출 예시\n",
    "Query에 따라 문서들을 tf-idf값으로 이루어진 벡터로 변형 및 정렬 가능.  \n",
    "문서를 유사도에 따라 정렬 (similarity *유사도* = proximity *거리* )  \n",
    "- 벡터간 거리를 이용하면 서로 다른 크기의 벡터를 가진 문서들에 적용이 어려움.\n",
    "- 벡터간 각도를 사용하면 한 직선 상의 문서들은 거리가 다름에도 각도가 0일 수 있음.\n",
    "- 이를 극복하기 위해 정규화 벡터의 *cosine 유사도* 사용\n",
    "\n",
    "- log TF를 구하고, 벡터를 normalization한다.\n",
    "- 이후 두 문서의 유사도를 구하기 위해서는 dot product 사용\n",
    "\n",
    "\n",
    "# 6. 문서 분류 (Text classification)\n",
    "---\n",
    "문서를 카테고리, 주제, 장르 등의 기준으로 분류하는 작업니다.\n",
    "- 스팸 필터링, 저자 확인, 언어 확인, 감성 분석\n",
    "- Input: \n",
    "    - 문서 (document): d\n",
    "    - 클래스 (장르) C = {c1,c2,...,cn}\n",
    "    \n",
    "- Output:\n",
    "    - 문서 d가 속하는 클래스 c ~ C\n",
    "\n",
    "규칙을 통하기보다는 기계학습 알고리즘을 이용해서 classifier 사용하는게 좋다.\n",
    "1. Naive Bayes\n",
    "    - 문서 d가 클래스 c에 속할 확률을 찾음\n",
    "    - 등장 단어를 바탕으로 어떤 클래스에 해당할지 계산\n",
    "    - *laplace smoothing*: 새로운 단어가 등장해도 해당 빈도에 1을 더해줘서 확률이 0이 되는걸 방지\n",
    "    \n",
    "    \n",
    "# 7. 감정 분석 Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
